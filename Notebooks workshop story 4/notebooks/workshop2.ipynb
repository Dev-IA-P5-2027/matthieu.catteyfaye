{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e8a2ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "de2f41ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quelques variables globales utiles\n",
    "PATH_TO_HAM_DIR = \"C:\\\\Users\\\\matth\\\\Documents\\\\GitHub\\\\matthieu.catteyfaye\\\\Notebooks workshop story 3\\\\data\\\\ham\"\n",
    "PATH_TO_SPAM_DIR = \"C:\\\\Users\\\\matth\\\\Documents\\\\GitHub\\\\matthieu.catteyfaye\\\\Notebooks workshop story 3\\\\data\\\\spam\"\n",
    "\n",
    "SPAM_TYPE = \"SPAM\"\n",
    "HAM_TYPE = \"HAM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "64fee78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#les tableaux X et Y seront de la meme taille et ordonnes\n",
    "X = [] # represente l'input Data (ici les mails)\n",
    "#indique s'il s'agit d'un mail ou non\n",
    "Y = [] #les etiquettes (labels) pour le training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e95946c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFilesFromDirectory(path, classification):\n",
    "    os.chdir(path)\n",
    "    files_name = os.listdir(path)\n",
    "    for current_file in files_name:\n",
    "        message = extract_mail_body(current_file)\n",
    "        X.append(message)\n",
    "        Y.append(classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e018319c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction de lecture du contenu d'un fichier texte donne.\n",
    "#ici, on fait un peu de traitement pour ne prendre en compte que le \"corps du mail\".\n",
    "# On ignorer les en-tetes des mails\n",
    "def extract_mail_body(file_name_str):\n",
    "    inBody = False\n",
    "    lines = []\n",
    "    file_descriptor = io.open(file_name_str,'r', encoding='latin1')\n",
    "    for line in file_descriptor:\n",
    "        if inBody:\n",
    "            lines.append(line)\n",
    "        elif line == '\\n':\n",
    "            inBody = True\n",
    "        message = '\\n'.join(lines)\n",
    "    file_descriptor.close()\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dab4b8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#appel de la fonction de chargement des mails (charger les mail normaux ensuite les SPAM)\n",
    "readFilesFromDirectory(PATH_TO_HAM_DIR, HAM_TYPE)\n",
    "readFilesFromDirectory(PATH_TO_SPAM_DIR, SPAM_TYPE)\n",
    "\n",
    "training_set = pd.DataFrame({'X': X, 'Y': Y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "52d189ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m vectorizer = CountVectorizer()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m counts = \u001b[43mvectorizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_set\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m classifier = MultinomialNB()\n\u001b[32m      5\u001b[39m targets = training_set[\u001b[33m'\u001b[39m\u001b[33mY\u001b[39m\u001b[33m'\u001b[39m].values\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matth\\Documents\\GitHub\\matthieu.catteyfaye\\.venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matth\\Documents\\GitHub\\matthieu.catteyfaye\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1377\u001b[39m, in \u001b[36mCountVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   1369\u001b[39m             warnings.warn(\n\u001b[32m   1370\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mUpper case characters found in\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1371\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m vocabulary while \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlowercase\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1372\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m is True. These entries will not\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1373\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m be matched with any documents\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1374\u001b[39m             )\n\u001b[32m   1375\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1377\u001b[39m vocabulary, X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.binary:\n\u001b[32m   1380\u001b[39m     X.data.fill(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\matth\\Documents\\GitHub\\matthieu.catteyfaye\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1283\u001b[39m, in \u001b[36mCountVectorizer._count_vocab\u001b[39m\u001b[34m(self, raw_documents, fixed_vocab)\u001b[39m\n\u001b[32m   1281\u001b[39m     vocabulary = \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[32m   1282\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[32m-> \u001b[39m\u001b[32m1283\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1284\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1285\u001b[39m         )\n\u001b[32m   1287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m indptr[-\u001b[32m1\u001b[39m] > np.iinfo(np.int32).max:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[32m   1288\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[31mValueError\u001b[39m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "counts = vectorizer.fit_transform(training_set['X'].values)\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "targets = training_set['Y'].values\n",
    "classifier.fit(counts, targets)\n",
    "\n",
    "\n",
    "examples = ['Free Viagra now!!!', \"Hi Bob, how about a game of golf tomorrow?\"]\n",
    "example_counts = vectorizer.transform(examples)\n",
    "predictions = classifier.predict(example_counts)\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
